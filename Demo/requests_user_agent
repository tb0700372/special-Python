# -*- coding: utf-8 -*-
# 测试浏览器请求头是否可用
import requests
from requests import RequestException
from lxml import etree
from time import sleep
import random

# 搭建浏览器用户池
user_agents = [
    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; rv:55.0) Gecko/20100101 Firefox/55.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:59.0) Gecko/20100101 Firefox/59.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:61.0) Gecko/20100101 Firefox/61.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:59.0) Gecko/20100101 Firefox/59.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:57.0) Gecko/20100101 Firefox/57.0',
    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:55.0) Gecko/20100101 Firefox/55.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 OPR/50.0.2762.67',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.45 Safari/537.36 OPR/53.0.2907.7 (Edition beta)',
]


def getUrl(url, agent):
    """URL请求
    需要导入库:
      import requests
      from requests import RequestException
    """
    headers = {'User-Agent': agent}  # 随机获取一个浏览器用户信息
    try:
        # html = requests.get(url, headers=headers, timeout=3, proxies={"https": "https://" + proxy})
        html = requests.get(url, headers=headers, timeout=3)
        print('getUrl.return')
        return html
    except RequestException as Er:
        print('getUrl.Er')
        return None


def parse(index, text):
    """页面解析
    需要导入库:
      from lxml import etree
    """
    page = etree.HTML(index)
    try:
        info = page.xpath(text)[0]
    except Exception:
        return None
    return info


def main(url):
    num = 0
    for i in user_agents:
        try:
            html = getUrl(url, i)
            if html:
                h1 = parse(html.text, '//div[@id="1"]//a/text()')
                num += 1
                print(h1, num, i)
        except Exception as Er:
            print(Er)
        sleep(1)
if __name__ == '__main__':
    url = 'https://www.baidu.com/s?wd=https%3A%2F%2Fblog.csdn.net%2Ftb0700372%2Farticle%2Fdetails%2F80604796&rsv_spt=1&rsv_iqid=0xe6e358950000c9dd&issp=1&f=8&rsv_bp=0&rsv_idx=2&ie=utf-8&rqlang=&tn=baiduhome_pg&ch=&rsv_enter=0&inputT=1207'  # url,判断ip是否有效
    main(url)
